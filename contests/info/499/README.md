## Description
In this experiment, users were presented with a cartoon and asked to rate the
caption. They had a choice between "unfunny", "somewhat funny" and "funny".

This data was collected by Robert Mankoff of the New York Times for the
weekly caption contest during the week of 2015-11-18. The link to this contest
was published in a video/blog post called [Cartoon Lounge: Show Me the Funny].
This is the New Yorker cartoon #499.

[Cartoon Lounge: Show Me the Funny]:http://www.newyorker.com/cartoons/bob-mankoff/cartoon-lounge-show-me-the-funny

## Numerics
(as of 2015-11-24 9:20pm CST)

* **Number of responses:** 103,306
* **Number of participants:** (may not be unique partcipants) 4,851
* **Time range:** 2015-11-18 00:19 to 2015-11-24 9:20
## Datasets provided
* **[participant-responses.csv]:** This includes all the responses collected.
  This data had been collected via two algorithms to choose which set of
  captions to present the user. One algorithm (the [Lil_UCB algorithm]) made
  decisions from data collected to determine which captions to rate and another
  algorithm sampled randomly (described in more detail below).
* **[participant-responses_RandomSampling.csv]:** This dataset was generated by
  selecting each caption uniformly without replacement at random from the full list and presenting
  it to the user for their judgement. Because random sampling was used, this
  dataset contains unbiased data -- no decisions were made one which question
  to show the user.
* **[participant-responses_LilUCB.csv]:** This dataset was generated by
  adaptively selecting which captions to query the user with according to the
  [Lil_UCB algorithm]. It contains biased data.

[Lil_UCB algorithm]:http://arxiv.org/abs/1312.7308
[participant-responses_LilUCB.csv]:individual_algorithm_responses/participant-responses_LilUCB.csv
[participant-responses_RandomSampling.csv]:individual_algorithm_responses/participant-responses_RandomSampling.csv
[participant-responses.csv]:participant-responses.csv

## CSV headers
* **Partipipant ID:** The ID assigned to each participant. Note this is
  assigned when the page is visited; if the same user visits the page twice,
  they will get two participant IDs.
* **Response Time (s):** How long the participant took to respond to the
  question. Network delay is accounted for
* **Network delay (s):** How long the question took to load.
* **Timestamp:** When the query was generated (and not when the query was
  answered)
* **Rating:** What the user rated the caption as. This can be either 1, 2 or 3
  depending on if the joke was unfunny, somewhat funny or funny respectively.
* **Alg label:** The algorithm responsible for showing the query. The random
  sampling is unbiased while "Lil_UCB" adaptively chooses the funniest caption.
* **Target:** The caption the user is asked to rate.

### Example query
![](query.png)

All possible captions are in captions.txt. From these captions, the algorithms
tried to find the funniest captions by user ratings.

